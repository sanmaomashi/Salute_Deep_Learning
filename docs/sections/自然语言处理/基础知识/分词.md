# 分词

?>分词是自然语言处理中一项重要的技术，其目的是将一个语句或篇章按照意义进行合理划分和标注。在中文中，由于汉字是无边界的，因此分词尤为重要，是中文处理的基础。

## 一、基于词典的分词算法（字符串匹配分词算法）

基于词典的分词算法是一种最基础的分词方法，它也被称为基于规则的分词方法。其基本思想是在一个预先定义好的词表或字典中查找划分文本的可能词语，然后按照词语的长度和位置信息进行识别。

具体来说，基于词典的分词算法的工作流程如下：

1. 构建词典：将大量文本中出现过的词语记入词典，包括单个汉字、词组等。构建词典一般需要领域知识和大量的数据支持。
2. 匹配词语： 对待分词句子进行遍历，依次提取出所有可能成词的子序列，并在词典中查找，判断该子序列是否在词典中存在。如果存在，则说明该序列是一个词语，可以进行标记。如果不存在，则需要进一步对子序列进行处理。
3. 处理非词典中的子序列：当遇到非词典中的子序列时，可以使用一些规则进行处理，比如拼音首字母、形态学等。也可以通过机器学习等方法，根据已知的切分结果，对未知词进行切分。
4. 输出切分结果：根据切分结果，以标记、空格等方式进行输出。

基于词典的分词算法的优点是速度较快、易于理解和实现，并且能够应对常用的词语和术语。但它也有一些缺点，比如对于新词或未知词没有很好的处理能力，同时对于歧义、多音字也容易出现错误。因此，基于词典的算法通常需要和其他算法相结合，如基于规则的分词算法、基于统计的分词算法等，以提高准确率和效果。

### 1. 正向最大匹配法 

正向最大匹配法基本思路是从句子的左端开始，从最大长度到最小长度依次尝试匹配词典中的词语，直到匹配到一个词或不能再匹配为止。具体来说，正向最大匹配法的步骤如下：

1. 构建词典： 通常情况下，使用哈希表等数据结构记录下所有可能出现的词语，并给词语标上标签，表示该词是否是完整的词语。
2. 遍历句子：正向最大匹配法从句子的左端开始扫描每个字符，并依次尝试匹配词典中的最长词语，即从词长最大的词语开始匹配。如果匹配成功（即匹配到了一个完整的词语），则将该词语加入分词结果，并从下一个字符继续匹配。如果匹配不成功，则去掉最右边的一个字符，继续尝试匹配。
3. 输出结果： 得到分词结果后，根据需求以标记、空格等方式进行输出。

正向最大匹配法是中文分词中最经典的一种算法，具有简单、高效等优点，在实际应用中比较常见。同时，该方法需要预先构建字典，对于包含大量生僻词或新词的场景，其分词效果可能不够理想。

下面是使用Python实现正向最大匹配法的简单代码示例：

```python
def forward_max_match(sentence, word_dict):
    max_word_len = max([len(word) for word in word_dict])  # 获得词典中最大词语的长度
    result = []
    while sentence:
        word = sentence[:max_word_len]
        while len(word) > 0:  # 从最大词长开始，依次减小尝试匹配
            if word in word_dict:  # 匹配到单词
                result.append(word)  # 加入分词结果
                break
            else:
                word = word[:-1]
        if not word:  # 当前字符无法匹配到词语，将其视为单个字符
            word = sentence[0]
            result.append(word)
        sentence = sentence[len(word):]  # 将已分词的字符串去掉，继续进行分词
    return result
```

例子：

```python
sentence = '我是中国人，小明是程序员。'
word_dict = ['我', '是', '中国人', '小明', '程序员']
result = forward_max_match(sentence, word_dict)
print(result)
```

输出：

```python
['我', '是', '中国人', '，', '小明', '是', '程序员', '。']
```

输出结果将句子划分成了一系列单词，其中标点符号会被作为单独的字符输出。当然，这个函数仅仅是一个简单的示例，可以根据自己的需求进行修改和优化。

### 2. 逆向最大匹配法

逆向最大匹配法从句子末尾开始，逐词向前匹配，找出最大匹配的词语。具体的实现步骤如下：

1. 选择一个合适的窗口大小作为初始值（通常为2-4个字）。
2. 从待分词文本的末尾开始，取出窗口内的词语，在已有的分词词典中进行匹配。如果没有匹配成功，则缩小窗口大小，继续匹配。
3. 重复步骤2，直到整个文本被分词完成。

逆向最大匹配法的优点是实现简单，适用于一些长度变化较大的中文文本。缺点是对于一些歧义词较多的文本，可能会出现分词错误。

例如："我是中国人"，若以3个字为窗口大小，则分词结果为"我是 / 中国人"，若缩小窗口大小为2个字，则分词结果为"我 / 是 / 中 / 国人"。

下面是Python实现逆向最大匹配法的代码：

```python
def reverse_max_match(text, dic):
    result = []  # 用来存储分词结果
    window_size = max(len(word) for word in dic)  # 窗口大小，取词典中最长词长度作为参考值
    index = len(text)  # 从文本末尾开始匹配
    while index > 0:
        word = None  # 存储匹配成功的词语
        for size in range(window_size, 0, -1):  # 从窗口最大尺寸开始逐渐减小
            print(size)
            if index - size < 0:  # 窗口超出文本边界
                continue
            piece = text[(index - size):index]  # 取出窗口内的词语
            if piece in dic:  # 词典中存在该词语
                word = piece
                result.append(word)
                index -= size  # 窗口向前移动
                break
        if word is None:  # 没有匹配成功的词语，即有未登录词
            result.append(text[index - 1])
            index -= 1
    return result[::-1]  # 返回逆序后的分词结果
```

代码中的 `sentence` 是要分词的文本，`word_dict` 是所有可能的词语集合。

下面是一个使用该函数的示例：

```python
sentence = '我是中国人，小明是程序员。'
word_dict = ['我', '是', '中国人', '小明', '程序员']  # 简单地设定词典
print(reverse_max_match(sentence, word_dict))
```

运行结果为：

```python
['我', '是', '中国人', '，', '小明', '是', '程序员', '。']
```

可以看出，该函数将文本分成了 9 个部分，其中一部分包含了两个词语。

### 3. 双向匹配分词法

双向匹配分词算法通过在待分词文本中同时从左向右和从右向左进行扫描，在两个方向上都匹配词典中的词，从而确定最优的分词位置。双向匹配分词算法具有简单高效的特点，常用于文本分析和自然语言处理领域。双向匹配分词算法的主要思想为：

从文本的左侧开始，每次匹配最长的词，直到分词结束。

从文本的右侧开始，每次匹配最长的词，直到分词结束。

同时在从左到右过程中记录所有可能的分词结果，在从右到左过程中与从左到右过程中的结果进行比较，确定最优的分词位置。

具体来说，双向匹配分词算法可以通过以下步骤实现：

1.读取待分词文本，确定分词范围。

2.分别从左侧和右侧开始扫描文本，匹配最长的词，并记录所有可能的分词结果。

3.将从左到右过程中的结果与从右到左过程中的结果进行比较，确定最优的分词位置。

4.输出分词结果。

需要注意的是，双向匹配分词算法并不能解决所有中文分词问题，在某些情况下可能会出现偏差或错误，因此在实际应用中需要结合其他分词算法进行优化和改进。

以下是一个简单的Python实现，使用词典列表来存储待匹配的词语，使用循环和切片操作来实现匹配和记录分词结果：

```python
# 从左到右匹配
def forward_match(text: str, word_set: set) -> list:
    res = []
    i = 0
    while i < len(text):
        longest_word = ''
        for j in range(i, len(text)):
            if text[i:j + 1] in word_set and len(text[i:j + 1]) > len(longest_word):
                longest_word = text[i:j + 1]
        if longest_word:
            res.append(longest_word)
            i += len(longest_word)
        else:
            res.append(text[i])
            i += 1
    return res


# 从右到左匹配
def backward_match(text: str, word_set: set) -> list:
    text = text[::-1]
    res = forward_match(text, word_set)
    return [word[::-1] for word in res[::-1]]


# 双向匹配分词
def bidirectional_match(text: str, word_set: set) -> list:
    forward_res = forward_match(text, word_set)
    backward_res = backward_match(text, word_set)
    if forward_res == backward_res:
        return forward_res
    else:
        return forward_res if len(forward_res) < len(backward_res) else backward_res


# 测试
if __name__ == '__main__':
    sentence = '我是中国人，小明是程序员。'
    word_dict = ['我', '是', '中国人', '小明', '程序员']
    res = bidirectional_match(sentence, word_dict)
    print(res)

```

输出分词结果：

```python
['我', '是', '中国人', '，', '小明', '是', '程序员', '。']
```

## 二、基于统计的分词算法

基于统计的分词方法是利用大量的文本语料库和统计模型来进行分词的一种方法，其基本思想是通过大量的训练数据和统计模型，来帮助计算机学习到在某种语言环境下，哪些字或词语更有可能出现在一起，并用此来进行分词。常见的基于统计的分词方法包括隐马尔科夫模型、最大熵模型、条件随机场等。

基于统计的分词方法通常需要大量的标注数据和计算资源，但其优点在于可以自动学习语言环境下哪些字或词语更有可能构成词语。但同时也存在一些局限性，例如对于新词或未知词的处理能力有限，且在特殊应用场景下容易出现错误的分词结果。因此，在实际应用中，也需要根据具体情况选择最合适的分词方法。

### 1. 隐马尔科夫模型(HMM)



### 2. CRF



### 3. SVM





## 三、基于深度学习的分词算法



## 四、具体分词工具

- 哈工大的LTP [HIT-SCIR/ltp](https://github.com/HIT-SCIR/ltp)
- jieba分词工具 [yanyiwu/cppjieba](https://link.zhihu.com/?target=https%3A//github.com/yanyiwu/cppjieba)
- 清华大学THULAC [https://github.com/thunlp/THULAC](https://link.zhihu.com/?target=https%3A//github.com/thunlp/THULAC)
- 斯坦福分词器 [https://nlp.stanford.edu/software/segmenter.shtml](https://link.zhihu.com/?target=https%3A//nlp.stanford.edu/software/segmenter.shtml)
- Hanlp分词 [hankcs/HanLP](https://link.zhihu.com/?target=https%3A//github.com/hankcs/HanLP)
- 字嵌入+Bi-LSTM+CRF分词器 [https://github.com/koth/kcws](https://link.zhihu.com/?target=https%3A//github.com/koth/kcws)
- ZPar分词器 [frcchang/zpar](https://link.zhihu.com/?target=https%3A//github.com/frcchang/zpar/releases)

## 五、存在问题

目前存在的问题目前中文分词难点主要有三个：

1、分词标准：比如人名，在哈工大的标准中姓和名是分开的，但在Hanlp中是合在一起的。这需要根据不同的需求制定不同的分词标准。

2、歧义：对同一个待切分字符串存在多个分词结果。

3、新词：也称未被词典收录的词。
